{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's assignment consists of two parts. The first part gives you an introduction to unsupervised learning. In particular, we focus on techniques for clustering and dimensionality reduction and how they can be applied to ecommerce data. As you work through the three clustering case studies, you will find yourself generating many intermediate datasets, trying different models, and tuning each model as you go along. There's a lot to keep track of.   \n",
    "\n",
    "This is where Part 2 comes in. It is in Part 2 that we introduce ideas of **workflow management** and **computational reproducibility**. Workflow management means organising your project directory to manage your analysis' artefacts (visualisations, processed datasets, notebooks and utility functions and experiment results). Ideally, your code for these should be clearly commented with well chosen names. Computational reproducibility means someone else (including future you!) being able to take just the code and data, and reproduce your project, from its results and models to visualisations etc. How one decides to practice workflow management and computational reproducibility can be quite a personal decision. Therefore, we provide guidelines, not rules. The most important is having a system rather than no system at all. \n",
    "\n",
    "**note about the week**   \n",
    "While week 1's assignment was guided, with specific instructions about what code to run, as we move on the assignments will involve less hand-holding. For this week, we include some instructions, but leave the specific implementations up to you. There are also many techniques we cover. Again, while we share some resources, we leave the bulk of the research and background reading up to you to manage for yourself. \n",
    "\n",
    "**recap of the objectives for the first 6 weeks:**  \n",
    "We aim to broadly cover a wide range of Machine Learning algorithms so that you can: \n",
    "- handle the technical demands of a 100E given some guidance on the right direction to take \n",
    "- can handle a technical job interview and get hired \n",
    "\n",
    "*materials for unsupervised learning adapted from William Thji* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I : Unsupervised Learning \n",
    "Unsupervised Learning refers to a set of machine learning techniques where no output variables (Y) are given. Only the input variables (X) are available and our job is to find patterns in X. You may read more about it from *pg 485 from Hastie and Tibshirani's Elements of Statistical Learning* available [here](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). \n",
    "\n",
    "ESL by Hastie et. al with be the primary reference for this week, although feel free to source for your own books and links. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short introduction to clustering \n",
    "Clustering puts datapoints into subsets so that datapoints within a cluster are more closely related to one another compared to datapoints in another cluster. More information is available from page 501 of *Elements of Statistical Learning*. \n",
    "\n",
    "Some quick points: \n",
    "- Clustering is extremely useful to many fields: \n",
    "    - Customer segmentation for personalised product recommendations\n",
    "    - Topic identification to relieve the need to manually vet documents \n",
    "    - Image or geo-spatial segmentation to optimised supply and demand (Gojek does this) \n",
    "    - and maybe most importantly, getting a sense of the data before starting to model it. \n",
    "\n",
    "- Some examples of clustering algorithms: \n",
    "    - KMeans\n",
    "    - Gaussian Mixture Models for drawing soft clustering boundaries instead of hard ones \n",
    "    - Hierarchical clustering\n",
    "    - DBScan for density-based clustering for anomaly detection \n",
    "    - Co-clustering\n",
    "    - Biclustering for analysing genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Case Study 1: Using PCA and clustering to create customer segments \n",
    "Context: The dataset we will be working with contains ecommerce transactions from a UK-based online retails store. The dataset is available on [Kaggle](https://www.kaggle.com/carrie1/ecommerce-data/home) or the UCI Machine Learning Repository. The dataset is quite small, so we have also included it inside the `data` folder inside this repo as `data/data.csv`. \n",
    "\n",
    "From the Kaggle website: \n",
    "\n",
    "\"This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('data/data.csv', encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data \n",
    "\n",
    "Some data types are muddled, there are duplicates, NA values and unreasonable values hiding in the columns \n",
    "\n",
    "1. Clean the dataset and save the output as an intermediate dataset \n",
    "2. List the steps taken to clean the data\n",
    "3. Extra: Encapsulate the steps inside their own functions so they can be reused. Organise the functions into their own library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering iteration #1 \n",
    "\n",
    "Inside the dataset, each row contains information about an ecommerce transation. However, we want to cluster the data by customers, which means each row should instead contain information about a customer. \n",
    "\n",
    "1. Reshape the data to look like the table below: \n",
    "![alt text](customer.png)\n",
    "\n",
    "2. `data.columns` should give you `['UnitPriceMean','UnitPriceStd','TotalQuantity','NoOfUniqueItems','NoOfInvoices','UniqueItemsPerInvoice','QuantityPerInvoice','SpendingPerInvoice']` \n",
    "\n",
    "3. `data.shape` should give you (4339, 8)\n",
    "4. Save this dataset as an intermediate dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering iteration #1 \n",
    "\n",
    "1. Normalise the dataset from the section above and create a pairplot of the data\n",
    "2. Apply hierarchical clustering to the dataset. [link to resource on hierarchical clustering]\n",
    "2. Experiment with different linkage algorithms. Visualise the resulting trees side-by-side. Which linkage algorithm works best? \n",
    "3. List two ways to improve the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means and GMM Clustering iteration #1 \n",
    "Apart from hierarchical clustering, we can also apply KMeans and Gaussian Mixture Models (GMM) on the data\n",
    "\n",
    "1. Implement K-means clustering on the data. You may want to create a dictonary or DataFrame to store the predicted labels for each value of `k` tried. For example, for `k = 4`, we get a dictionary of `{0:50, 1:100, 2:40, 3:20}` where the keys are values of `k` and the values are their associated label counts. \n",
    "2. For each value of `k`, print the number of members in in each label class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model improvement iteration #1 \n",
    "\n",
    "1. One way of improving our model is to identify and remove outliers in the data. Clustering can help us do this. Using a clustering algorithm, identify and remove potential outliers in the data. More resources on how to implement clustering to identify outliers can be found here [link]\n",
    "2. Create two visualisations, one of the data before applying clustering and one of the data after clustering. \n",
    "2. Identify and remove outliers from the data WITHOUT using clustering algorithm\n",
    "3. Create a boxplot of each column in the dataset after outliers have been removed. \n",
    "4. Save the normalised dataset with no outliers as an intermediate dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering iteration #2 \n",
    "\n",
    "1. Using the dataset created from `Model Improvement Iteration #1`, implement hierarchical clustering again. Are there improvements? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means and Gaussian Mixture Model (GMM) Clustering iteration #2 \n",
    "\n",
    "1. Using the dataset created from `Model Improvement Iteration #1`, implement `K-means` and `GMM` clustering again. \n",
    "2. Run multiple experiments by testing k=1,...,20 for K-means and components=1,...,20 for Gaussian Mixture Models \n",
    "3. Plot a graph of the silhouette score against k for K-means and plot a graph of the silhouette score against number of components for GMM \n",
    "4. How would you select the optimal number of clusters using the silhouette score? \n",
    "5. Explain one other way of validating your clusters \n",
    "6. Plot a graph of the cluster center coordinates against number of clusters for K-means. \n",
    "Does this help you choose the best number of clusters? \n",
    "7. Plot a graph of the means of each component GMM distribution again the number of components for GMM. Does this help you choose the best number of clusters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means and GMM Clustering iteration #3\n",
    "\n",
    "1. Choose a subset of columns from the ecommerce dataset with the outliers removed \n",
    "2. Run multiple experiments by testing k=1,...,20 for K-means and components=1,...,20 for Gaussian Mixture Models \n",
    "3. Plot a graph of the silhouette score against k for K-means and plot a graph of the silhouette score against number of components for GMM\n",
    "4. Based on the silhouette scores for the GMM model, choose the optimal number of components for this dataset\n",
    "5. For each mixture component in the GMM model from 4, plot their covariances for each column in the dataset. Based on this plot, which columns should we keep? \n",
    "6. Create a DataFrame with only the columns you chose to keep from 4. Save this DataFrame as an intermediate dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM Clustering iteration #4 \n",
    "\n",
    "1. Based on the intermediate datset created from iteration #3, again run n_components=1,...,20 for the GMM\n",
    "2. Plot the silhouette scores against n_components. Based on this plot, choose the optimal value for n_components and fit a GMM model using this optimal value. What are the value counts of each mixture component? \n",
    "3. Create a DataFrame with the columns you chose to keep as columns and the means of the mixture components as rows. \n",
    "4. Save this final DataFrame. These are the customer segments that will be used for Case Study 2\n",
    "5. Create a heatmap of the DataFrame of customer segments. What does this tell us about the information contained within each mixture component? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the outliers \n",
    "\n",
    "1. Do the outliers themselves cluster into subgroups, or are they distributed randomly?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for Dimensionality Reduction \n",
    "\n",
    "1. Using the intermediate dataset that was normalised with outliers removed, construct a pairplot again. How is it different from the first plot and how might you interpret it? \n",
    "2. Apply PCA to the normalised dataset with outliers removed. More information on PCA here [link]\n",
    "3. Create a plot of cumulative explained variance and number of components. How does this inform you about the best number of components to select? \n",
    "4. Create a plot of PC0 against PC1, coloured by the GMM's predictions on the normalised dataset with outliers removed for n_components =7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
